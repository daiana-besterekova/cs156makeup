{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "765e5dc7",
   "metadata": {},
   "source": [
    "# Pre class work "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aee4986",
   "metadata": {},
   "source": [
    "### Q1 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e179132",
   "metadata": {},
   "source": [
    "1. What data is it training on? - MNIST dataset\n",
    "\n",
    "2. What is it learning to do? - It is learning to predict handwritten numbers\n",
    "\n",
    "3. How many layers does it have? - It seems to have 4 layers\n",
    "\n",
    "4. How many units in the layers? - 784, 128, 64 and 10\n",
    "\n",
    "5. What are the units activation functions? - sigmoid\n",
    "\n",
    "6. How are the units connected to each other (i.e., how are the weights set initially?) - To update the function, Binary Entropy is used \n",
    "\n",
    "7. What is the output? - a 10-dimensional vector with values that corresponds to a particular value which increases the probability of the input being identified as a number\n",
    "\n",
    "8. How well is it performing the classification? - The accuracy is low "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7c1153",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "(x_train, y_train), (x_val, y_val) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "y_train = to_categorical(y_train)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='sigmoid'),\n",
    "    Dense(64, activation='sigmoid'),\n",
    "    Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss=BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10)\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf80540a",
   "metadata": {},
   "source": [
    "https://docs.google.com/drawings/d/1pi6SJ03tvTCvKmbQV3Ee5WY_Ki3gUOPnKJtvvaScYVU/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fd0e52",
   "metadata": {},
   "source": [
    "### Q2\n",
    "\n",
    "1. What is it learning to do? - Identify rocks and metal cylinders\n",
    "2. How many layers does it have? - 2 layers, the first one is input, the second one is output \n",
    "3. How many units in the layers? - the first layer has 60 variables, while the second one has 1\n",
    "4. What are the units activation functions? - RELU function \n",
    "5. How are the units connected to each other (i.e., how are the weights set initially?) - Using stochastic gradient descent \n",
    "6. What is the output? -  Output is a binary number - 1 or 0 \n",
    "7. What do the functions predict() and train_weights() do? - train_weighs() calculates weight values using SGD, predict() predicts output given input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e96fe530",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sonar.all-data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/5k/0qz1zbqx0cx5b4hzqlkt8fl00000gn/T/ipykernel_31952/1172253738.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;31m# load and prepare data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sonar.all-data.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mstr_column_to_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/5k/0qz1zbqx0cx5b4hzqlkt8fl00000gn/T/ipykernel_31952/1172253738.py\u001b[0m in \u001b[0;36mload_csv\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m                 \u001b[0mcsv_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsv_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sonar.all-data.csv'"
     ]
    }
   ],
   "source": [
    "#retrieved from https://machinelearningmastery.com/implement-perceptron-algorithm-scratch-python/\n",
    "#Jason Brownlee \n",
    "\n",
    "# Perceptron Algorithm on the Sonar Dataset\n",
    "from random import seed\n",
    "from random import randrange\n",
    "from csv import reader\n",
    "\n",
    "# Load a CSV file\n",
    "def load_csv(filename):\n",
    "\tdataset = list()\n",
    "\twith open(filename, 'r') as file:\n",
    "\t\tcsv_reader = reader(file)\n",
    "\t\tfor row in csv_reader:\n",
    "\t\t\tif not row:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tdataset.append(row)\n",
    "\treturn dataset\n",
    "\n",
    "# Convert string column to float\n",
    "def str_column_to_float(dataset, column):\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = float(row[column].strip())\n",
    "\n",
    "# Convert string column to integer\n",
    "def str_column_to_int(dataset, column):\n",
    "\tclass_values = [row[column] for row in dataset]\n",
    "\tunique = set(class_values)\n",
    "\tlookup = dict()\n",
    "\tfor i, value in enumerate(unique):\n",
    "\t\tlookup[value] = i\n",
    "\tfor row in dataset:\n",
    "\t\trow[column] = lookup[row[column]]\n",
    "\treturn lookup\n",
    "\n",
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor i in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    "\n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    "\n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores\n",
    "\n",
    "# Make a prediction with weights\n",
    "def predict(row, weights):\n",
    "\tactivation = weights[0]\n",
    "\tfor i in range(len(row)-1):\n",
    "\t\tactivation += weights[i + 1] * row[i]\n",
    "\treturn 1.0 if activation >= 0.0 else 0.0\n",
    "\n",
    "# Estimate Perceptron weights using stochastic gradient descent\n",
    "def train_weights(train, l_rate, n_epoch):\n",
    "\tweights = [0.0 for i in range(len(train[0]))]\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tfor row in train:\n",
    "\t\t\tprediction = predict(row, weights)\n",
    "\t\t\terror = row[-1] - prediction\n",
    "\t\t\tweights[0] = weights[0] + l_rate * error\n",
    "\t\t\tfor i in range(len(row)-1):\n",
    "\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n",
    "\treturn weights\n",
    "\n",
    "# Perceptron Algorithm With Stochastic Gradient Descent\n",
    "def perceptron(train, test, l_rate, n_epoch):\n",
    "\tpredictions = list()\n",
    "\tweights = train_weights(train, l_rate, n_epoch)\n",
    "\tfor row in test:\n",
    "\t\tprediction = predict(row, weights)\n",
    "\t\tpredictions.append(prediction)\n",
    "\treturn(predictions)\n",
    "\n",
    "# Test the Perceptron algorithm on the sonar dataset\n",
    "seed(1)\n",
    "# load and prepare data\n",
    "filename = 'sonar.all-data.csv'\n",
    "dataset = load_csv(filename)\n",
    "for i in range(len(dataset[0])-1):\n",
    "\tstr_column_to_float(dataset, i)\n",
    "# convert string class to integers\n",
    "str_column_to_int(dataset, len(dataset[0])-1)\n",
    "# evaluate algorithm\n",
    "n_folds = 3\n",
    "l_rate = 0.01\n",
    "n_epoch = 500\n",
    "scores = evaluate_algorithm(dataset, perceptron, n_folds, l_rate, n_epoch)\n",
    "print('Scores: %s' % scores)\n",
    "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f03641",
   "metadata": {},
   "source": [
    "https://docs.google.com/drawings/d/1Dgf02p7DlsqDTypx0Tfr1EtUBbsvER8UoiSy6ONBEQI/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564aefa8",
   "metadata": {},
   "source": [
    "# Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbbd7b6",
   "metadata": {},
   "source": [
    "The insightful comment was given by Abeera during the poll discussion regarding the activation function. When looking at the activation equation, we often might think that the function is indeed linear; however, the function underneath the activation function is not linear. This is very cool to know because we are simply representing activation functions by hiding the functions that form it. \n",
    "\n",
    "Abdullah's question was interesting to consider as well: Can we take the middle output (layer) and process that? More like can you use the middle values in the layers to come to an output? As professor explained, the artificial neural networks try to model humans' biological models. In the ANN, we can use timing as an example, where all unites are used. However, in the biological network, the representation is much more complex and different where values float around and do not have a distinct representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a6eed7",
   "metadata": {},
   "source": [
    "# Suggestion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f51d2cc",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/activation-functions-neural-networks/\n",
    "    \n",
    "This article provides clarification regarding activation functions in Neural Networks. It covers elements of neural network, the reasons behind using an activation function, and different variants of activation functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ecf06d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
